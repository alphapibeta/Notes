Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 5414 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
..
==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.
..50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 51200.4 ms
==PROF== Disconnected from process 5414
[5414] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 1023, 50)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ----------------
    Metric Name               Metric Unit     Metric Value
    ----------------------- ------------- ----------------
    DRAM Frequency          cycle/nsecond             6.79
    SM Frequency            cycle/usecond           960.80
    Elapsed Cycles                  cycle    5,716,550,623
    Memory Throughput                   %            24.60
    DRAM Throughput                     %             0.69
    Duration                       second             5.95
    L1/TEX Cache Throughput             %            24.61
    L2 Cache Throughput                 %             4.77
    SM Active Cycles                cycle 5,711,779,682.70
    Compute (SM) Throughput             %            37.35
    ----------------------- ------------- ----------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                             52,326,450
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,326,450
    Waves Per SM                                          109,013.44
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.83
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 6101 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
..
==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.
..50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 29846.3 ms
==PROF== Disconnected from process 6101
[6101] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 512, 50)x(1, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ----------------
    Metric Name               Metric Unit     Metric Value
    ----------------------- ------------- ----------------
    DRAM Frequency          cycle/nsecond             6.79
    SM Frequency            cycle/usecond           974.87
    Elapsed Cycles                  cycle    3,207,233,768
    Memory Throughput                   %            21.94
    DRAM Throughput                     %             1.25
    Duration                       second             3.29
    L1/TEX Cache Throughput             %            32.80
    L2 Cache Throughput                 %             7.03
    SM Active Cycles                cycle 3,206,885,556.57
    Compute (SM) Throughput             %            33.31
    ----------------------- ------------- ----------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                             26,188,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                              54,560
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 93.75%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 2      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.85
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 4 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x4
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 6308 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 17011 ms
==PROF== Disconnected from process 6308
[6308] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 256, 50)x(1, 4, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ----------------
    Metric Name               Metric Unit     Metric Value
    ----------------------- ------------- ----------------
    DRAM Frequency          cycle/nsecond             6.79
    SM Frequency            cycle/usecond           974.81
    Elapsed Cycles                  cycle    1,649,722,856
    Memory Throughput                   %            27.37
    DRAM Throughput                     %             2.43
    Duration                       second             1.69
    L1/TEX Cache Throughput             %            54.73
    L2 Cache Throughput                 %            11.97
    SM Active Cycles                cycle 1,649,401,080.13
    Compute (SM) Throughput             %            32.38
    ----------------------- ------------- ----------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     4
    Function Cache Configuration                     CachePreferNone
    Grid Size                                             13,094,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                              27,280
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 87.5%                                                                                           
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 4      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.85
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 8 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x8
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 6441 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 10721.5 ms
==PROF== Disconnected from process 6441
[6441] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 128, 50)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.38
    Elapsed Cycles                  cycle    887,103,063
    Memory Throughput                   %          44.85
    DRAM Throughput                     %           4.54
    Duration                      msecond         909.78
    L1/TEX Cache Throughput             %          89.69
    L2 Cache Throughput                 %          19.56
    SM Active Cycles                cycle 886,438,638.07
    Compute (SM) Throughput             %          30.13
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              6,547,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                              13,640
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 75%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.86
    Achieved Active Warps Per SM           warp        15.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 16 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x16
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 6551 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 9558.65 ms
==PROF== Disconnected from process 6551
[6551] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 64, 50)x(1, 16, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         974.14
    Elapsed Cycles                  cycle    744,549,806
    Memory Throughput                   %          49.79
    DRAM Throughput                     %           5.64
    Duration                      msecond         763.50
    L1/TEX Cache Throughput             %          99.58
    L2 Cache Throughput                 %          24.25
    SM Active Cycles                cycle 743,745,692.47
    Compute (SM) Throughput             %          17.95
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              3,273,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               6,820
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.92
    Achieved Active Warps Per SM           warp        15.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 32 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x32
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 6674 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 9553.61 ms
==PROF== Disconnected from process 6674
[6674] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 32, 50)x(1, 32, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.95
    Elapsed Cycles                  cycle    745,492,504
    Memory Throughput                   %          48.27
    DRAM Throughput                     %           5.51
    Duration                      msecond         764.64
    L1/TEX Cache Throughput             %          96.54
    L2 Cache Throughput                 %          44.29
    SM Active Cycles                cycle 745,901,114.70
    Compute (SM) Throughput             %           8.96
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              1,636,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               3,410
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.96
    Achieved Active Warps Per SM           warp        15.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 64 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x64
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 6779 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 9725.6 ms
==PROF== Disconnected from process 6779
[6779] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 16, 50)x(1, 64, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         975.46
    Elapsed Cycles                  cycle    759,593,271
    Memory Throughput                   %          55.31
    DRAM Throughput                     %           5.38
    Duration                      msecond         778.70
    L1/TEX Cache Throughput             %          94.85
    L2 Cache Throughput                 %          55.31
    SM Active Cycles                cycle 759,416,545.53
    Compute (SM) Throughput             %           8.79
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                818,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.89
    Achieved Active Warps Per SM           warp        31.00
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 128 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x128
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 6892 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 9782.43 ms
==PROF== Disconnected from process 6892
[6892] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 8, 50)x(1, 128, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         975.00
    Elapsed Cycles                  cycle    767,130,187
    Memory Throughput                   %          60.36
    DRAM Throughput                     %           5.28
    Duration                      msecond         786.80
    L1/TEX Cache Throughput             %          93.98
    L2 Cache Throughput                 %          60.36
    SM Active Cycles                cycle 767,038,257.97
    Compute (SM) Throughput             %           8.70
    ----------------------- ------------- --------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L2 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.94
    Achieved Active Warps Per SM           warp        31.34
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 256 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x256
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 7006 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 9826.92 ms
==PROF== Disconnected from process 7006
[7006] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 4, 50)x(1, 256, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.95
    Elapsed Cycles                  cycle    769,123,583
    Memory Throughput                   %          60.77
    DRAM Throughput                     %           7.10
    Duration                      msecond         788.88
    L1/TEX Cache Throughput             %          94.65
    L2 Cache Throughput                 %          60.77
    SM Active Cycles                cycle 769,121,646.90
    Compute (SM) Throughput             %           8.68
    ----------------------- ------------- --------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L2 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.17
    Achieved Active Warps Per SM           warp        31.41
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 512 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x512
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 7115 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 9820.92 ms
==PROF== Disconnected from process 7115
[7115] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 2, 50)x(1, 512, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         975.02
    Elapsed Cycles                  cycle    773,367,921
    Memory Throughput                   %          62.07
    DRAM Throughput                     %           7.57
    Duration                      msecond         793.18
    L1/TEX Cache Throughput             %          96.89
    L2 Cache Throughput                 %          62.07
    SM Active Cycles                cycle 773,258,264.30
    Compute (SM) Throughput             %           8.63
    ----------------------- ------------- --------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L2 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,300
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.03
    Achieved Active Warps Per SM           warp        31.37
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1 1024 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1x1024
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 7219 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 9976.44 ms
==PROF== Disconnected from process 7219
[7219] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1023, 1, 50)x(1, 1024, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.99
    Elapsed Cycles                  cycle    780,695,290
    Memory Throughput                   %          62.58
    DRAM Throughput                     %           7.49
    Duration                      msecond         800.72
    L1/TEX Cache Throughput             %          97.88
    L2 Cache Throughput                 %          62.58
    SM Active Cycles                cycle 780,387,361.83
    Compute (SM) Throughput             %           8.55
    ----------------------- ------------- --------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L2 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,150
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.41
    Achieved Active Warps Per SM           warp        30.21
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 7323 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
..
==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.
..50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 28476.4 ms
==PROF== Disconnected from process 7323
[7323] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 1023, 50)x(2, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ----------------
    Metric Name               Metric Unit     Metric Value
    ----------------------- ------------- ----------------
    DRAM Frequency          cycle/nsecond             6.79
    SM Frequency            cycle/usecond           974.59
    Elapsed Cycles                  cycle    3,048,551,436
    Memory Throughput                   %            23.09
    DRAM Throughput                     %             1.32
    Duration                       second             3.13
    L1/TEX Cache Throughput             %            23.41
    L2 Cache Throughput                 %             5.31
    SM Active Cycles                cycle 3,047,304,084.17
    Compute (SM) Throughput             %            35.06
    ----------------------- ------------- ----------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     2
    Function Cache Configuration                     CachePreferNone
    Grid Size                                             26,188,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                              54,560
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 93.75%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 2      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.84
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 7488 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 17432.7 ms
==PROF== Disconnected from process 7488
[7488] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 512, 50)x(2, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ----------------
    Metric Name               Metric Unit     Metric Value
    ----------------------- ------------- ----------------
    DRAM Frequency          cycle/nsecond             6.79
    SM Frequency            cycle/usecond           974.44
    Elapsed Cycles                  cycle    1,698,714,374
    Memory Throughput                   %            20.74
    DRAM Throughput                     %             2.36
    Duration                       second             1.74
    L1/TEX Cache Throughput             %            32.25
    L2 Cache Throughput                 %             7.71
    SM Active Cycles                cycle 1,697,735,630.07
    Compute (SM) Throughput             %            31.49
    ----------------------- ------------- ----------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     4
    Function Cache Configuration                     CachePreferNone
    Grid Size                                             13,107,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                           27,306.67
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 87.5%                                                                                           
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 4      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.86
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 4 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x4
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 7617 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 10713.4 ms
==PROF== Disconnected from process 7617
[7617] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 256, 50)x(2, 4, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.99
    Elapsed Cycles                  cycle    878,278,053
    Memory Throughput                   %          26.63
    DRAM Throughput                     %           4.57
    Duration                      msecond         900.81
    L1/TEX Cache Throughput             %          53.26
    L2 Cache Throughput                 %          13.07
    SM Active Cycles                cycle 878,368,503.60
    Compute (SM) Throughput             %          30.44
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              6,553,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                           13,653.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 75%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.86
    Achieved Active Warps Per SM           warp        15.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 8 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x8
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 7728 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 7325 ms
==PROF== Disconnected from process 7728
[7728] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 128, 50)x(2, 8, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.87
    Elapsed Cycles                  cycle    473,575,985
    Memory Throughput                   %          43.56
    DRAM Throughput                     %           8.50
    Duration                      msecond         485.78
    L1/TEX Cache Throughput             %          87.12
    L2 Cache Throughput                 %          22.12
    SM Active Cycles                cycle 473,552,764.57
    Compute (SM) Throughput             %          28.22
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              3,276,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            6,826.67
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.87
    Achieved Active Warps Per SM           warp        15.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 16 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x16
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 7827 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 6705.58 ms
==PROF== Disconnected from process 7827
[7827] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 64, 50)x(2, 16, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         972.16
    Elapsed Cycles                  cycle    390,111,393
    Memory Throughput                   %          49.66
    DRAM Throughput                     %          10.75
    Duration                      msecond         400.13
    L1/TEX Cache Throughput             %          99.31
    L2 Cache Throughput                 %          30.20
    SM Active Cycles                cycle 388,963,308.43
    Compute (SM) Throughput             %          17.18
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              1,638,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            3,413.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.92
    Achieved Active Warps Per SM           warp        15.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 32 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x32
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 7933 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 6736.52 ms
==PROF== Disconnected from process 7933
[7933] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 32, 50)x(2, 32, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.81
    SM Frequency            cycle/usecond         972.60
    Elapsed Cycles                  cycle    399,982,156
    Memory Throughput                   %          48.86
    DRAM Throughput                     %          10.28
    Duration                      msecond         409.15
    L1/TEX Cache Throughput             %          97.42
    L2 Cache Throughput                 %          48.86
    SM Active Cycles                cycle 397,174,193.53
    Compute (SM) Throughput             %          16.79
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                819,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.12
    Achieved Active Warps Per SM           warp        31.40
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 64 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x64
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8064 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 6741.38 ms
==PROF== Disconnected from process 8064
[8064] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 16, 50)x(2, 64, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         970.26
    Elapsed Cycles                  cycle    400,473,986
    Memory Throughput                   %          50.21
    DRAM Throughput                     %          10.16
    Duration                      msecond         410.42
    L1/TEX Cache Throughput             %          97.26
    L2 Cache Throughput                 %          50.21
    SM Active Cycles                cycle 398,244,569.63
    Compute (SM) Throughput             %          16.78
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.78
    Achieved Active Warps Per SM           warp        31.61
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 128 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x128
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8156 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 6721.27 ms
==PROF== Disconnected from process 8156
[8156] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 8, 50)x(2, 128, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         970.07
    Elapsed Cycles                  cycle    399,500,913
    Memory Throughput                   %          49.00
    DRAM Throughput                     %          10.53
    Duration                      msecond         409.81
    L1/TEX Cache Throughput             %          97.42
    L2 Cache Throughput                 %          49.00
    SM Active Cycles                cycle 397,497,073.83
    Compute (SM) Throughput             %          16.81
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.58
    Achieved Active Warps Per SM           warp        31.55
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 256 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x256
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8255 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 6760.58 ms
==PROF== Disconnected from process 8255
[8255] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 4, 50)x(2, 256, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         975.20
    Elapsed Cycles                  cycle    403,638,623
    Memory Throughput                   %          52.26
    DRAM Throughput                     %          15.52
    Duration                      msecond         413.90
    L1/TEX Cache Throughput             %          95.51
    L2 Cache Throughput                 %          52.26
    SM Active Cycles                cycle 403,453,230.50
    Compute (SM) Throughput             %          16.56
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.04
    Achieved Active Warps Per SM           warp        31.37
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 2 512 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 2x512
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8347 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 7027.38 ms
==PROF== Disconnected from process 8347
[8347] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (512, 2, 50)x(2, 512, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         970.24
    Elapsed Cycles                  cycle    426,949,201
    Memory Throughput                   %          60.31
    DRAM Throughput                     %          27.70
    Duration                      msecond         437.48
    L1/TEX Cache Throughput             %          90.75
    L2 Cache Throughput                 %          60.31
    SM Active Cycles                cycle 423,687,844.97
    Compute (SM) Throughput             %          15.75
    ----------------------- ------------- --------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L2 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.97
    Achieved Active Warps Per SM           warp        31.35
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 4 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 4x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8452 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 17399.1 ms
==PROF== Disconnected from process 8452
[8452] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (256, 1023, 50)x(4, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ----------------
    Metric Name               Metric Unit     Metric Value
    ----------------------- ------------- ----------------
    DRAM Frequency          cycle/nsecond             6.79
    SM Frequency            cycle/usecond           974.26
    Elapsed Cycles                  cycle    1,698,079,736
    Memory Throughput                   %            20.73
    DRAM Throughput                     %             2.36
    Duration                       second             1.74
    L1/TEX Cache Throughput             %            21.36
    L2 Cache Throughput                 %             5.97
    SM Active Cycles                cycle 1,696,771,881.83
    Compute (SM) Throughput             %            31.48
    ----------------------- ------------- ----------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     4
    Function Cache Configuration                     CachePreferNone
    Grid Size                                             13,094,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                              27,280
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 87.5%                                                                                           
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 4      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.85
    Achieved Active Warps Per SM           warp        15.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 4 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 4x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8576 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 11149 ms
==PROF== Disconnected from process 8576
[8576] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (256, 512, 50)x(4, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         973.82
    Elapsed Cycles                  cycle    934,903,286
    Memory Throughput                   %          18.86
    DRAM Throughput                     %           4.29
    Duration                      msecond         958.80
    L1/TEX Cache Throughput             %          29.85
    L2 Cache Throughput                 %           8.76
    SM Active Cycles                cycle 933,665,278.83
    Compute (SM) Throughput             %          28.63
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              6,553,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                           13,653.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 75%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.87
    Achieved Active Warps Per SM           warp        15.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 4 4 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 4x4
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8690 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 7493.51 ms
==PROF== Disconnected from process 8690
[8690] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (256, 256, 50)x(4, 4, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         972.43
    Elapsed Cycles                  cycle    492,238,324
    Memory Throughput                   %          24.95
    DRAM Throughput                     %           8.15
    Duration                      msecond         504.93
    L1/TEX Cache Throughput             %          49.90
    L2 Cache Throughput                 %          14.70
    SM Active Cycles                cycle 490,959,713.30
    Compute (SM) Throughput             %          27.22
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              3,276,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            6,826.67
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.87
    Achieved Active Warps Per SM           warp        15.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 4 8 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 4x8
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8796 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5749.37 ms
==PROF== Disconnected from process 8796
[8796] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (256, 128, 50)x(4, 8, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         970.50
    Elapsed Cycles                  cycle    273,777,501
    Memory Throughput                   %          39.69
    DRAM Throughput                     %          14.73
    Duration                      msecond         280.80
    L1/TEX Cache Throughput             %          79.38
    L2 Cache Throughput                 %          25.42
    SM Active Cycles                cycle 272,524,559.13
    Compute (SM) Throughput             %          24.52
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              1,638,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            3,413.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.88
    Achieved Active Warps Per SM           warp        15.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 4 16 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 4x16
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8891 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5219.05 ms
==PROF== Disconnected from process 8891
[8891] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (256, 64, 50)x(4, 16, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.74
    Elapsed Cycles                  cycle    218,338,742
    Memory Throughput                   %          49.78
    DRAM Throughput                     %          19.61
    Duration                      msecond         224.00
    L1/TEX Cache Throughput             %          99.57
    L2 Cache Throughput                 %          31.54
    SM Active Cycles                cycle 218,349,161.43
    Compute (SM) Throughput             %          30.61
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                819,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        30.90
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 4 32 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 4x32
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 8968 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5307.1 ms
==PROF== Disconnected from process 8968
[8968] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (256, 32, 50)x(4, 32, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.99
    Elapsed Cycles                  cycle    220,506,559
    Memory Throughput                   %          49.51
    DRAM Throughput                     %          18.80
    Duration                      msecond         226.16
    L1/TEX Cache Throughput             %          99.02
    L2 Cache Throughput                 %          37.33
    SM Active Cycles                cycle 220,507,172.23
    Compute (SM) Throughput             %          30.31
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.36
    Achieved Active Warps Per SM           warp        31.16
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 4 64 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 4x64
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 9042 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5415.36 ms
==PROF== Disconnected from process 9042
[9042] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (256, 16, 50)x(4, 64, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.97
    Elapsed Cycles                  cycle    220,374,252
    Memory Throughput                   %          49.40
    DRAM Throughput                     %          18.97
    Duration                      msecond         226.03
    L1/TEX Cache Throughput             %          98.79
    L2 Cache Throughput                 %          37.91
    SM Active Cycles                cycle 220,354,983.97
    Compute (SM) Throughput             %          30.33
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.46
    Achieved Active Warps Per SM           warp        30.87
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 4 128 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 4x128
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 9116 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5306.39 ms
==PROF== Disconnected from process 9116
[9116] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (256, 8, 50)x(4, 128, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.94
    Elapsed Cycles                  cycle    222,285,670
    Memory Throughput                   %          48.96
    DRAM Throughput                     %          21.44
    Duration                      msecond         228.00
    L1/TEX Cache Throughput             %          97.92
    L2 Cache Throughput                 %          36.20
    SM Active Cycles                cycle 222,277,537.80
    Compute (SM) Throughput             %          30.07
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.51
    Achieved Active Warps Per SM           warp        30.56
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 4 256 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 4x256
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 9208 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5342.38 ms
==PROF== Disconnected from process 9208
[9208] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (256, 4, 50)x(4, 256, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         975.13
    Elapsed Cycles                  cycle    232,711,870
    Memory Throughput                   %          46.36
    DRAM Throughput                     %          28.06
    Duration                      msecond         238.65
    L1/TEX Cache Throughput             %          92.72
    L2 Cache Throughput                 %          36.14
    SM Active Cycles                cycle 232,367,661.63
    Compute (SM) Throughput             %          28.72
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.24
    Achieved Active Warps Per SM           warp        30.80
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 8 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 8x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 9300 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 12189.9 ms
==PROF== Disconnected from process 9300
[9300] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (128, 1023, 50)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ----------------
    Metric Name               Metric Unit     Metric Value
    ----------------------- ------------- ----------------
    DRAM Frequency          cycle/nsecond             6.79
    SM Frequency            cycle/usecond           973.95
    Elapsed Cycles                  cycle    1,058,161,990
    Memory Throughput                   %            16.64
    DRAM Throughput                     %             3.79
    Duration                       second             1.09
    L1/TEX Cache Throughput             %            17.69
    L2 Cache Throughput                 %             6.33
    SM Active Cycles                cycle 1,056,992,702.47
    Compute (SM) Throughput             %            25.27
    ----------------------- ------------- ----------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              6,547,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                              13,640
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 75%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.88
    Achieved Active Warps Per SM           warp        15.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 8 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 8x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 9441 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 8260.66 ms
==PROF== Disconnected from process 9441
[9441] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (128, 512, 50)x(8, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         973.08
    Elapsed Cycles                  cycle    587,424,258
    Memory Throughput                   %          15.02
    DRAM Throughput                     %           6.83
    Duration                      msecond         602.41
    L1/TEX Cache Throughput             %          24.65
    L2 Cache Throughput                 %           9.34
    SM Active Cycles                cycle 586,292,474.13
    Compute (SM) Throughput             %          22.80
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              3,276,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            6,826.67
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.89
    Achieved Active Warps Per SM           warp        15.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 8 4 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 8x4
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 9563 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 6018.69 ms
==PROF== Disconnected from process 9563
[9563] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (128, 256, 50)x(8, 4, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         972.27
    Elapsed Cycles                  cycle    316,897,048
    Memory Throughput                   %          20.37
    DRAM Throughput                     %          12.66
    Duration                      msecond         325.00
    L1/TEX Cache Throughput             %          40.74
    L2 Cache Throughput                 %          15.21
    SM Active Cycles                cycle 315,991,113.93
    Compute (SM) Throughput             %          21.15
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              1,638,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            3,413.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.90
    Achieved Active Warps Per SM           warp        15.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 8 8 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 8x8
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 9697 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5045.21 ms
==PROF== Disconnected from process 9697
[9697] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (128, 128, 50)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.89
    Elapsed Cycles                  cycle    196,036,759
    Memory Throughput                   %          32.87
    DRAM Throughput                     %          20.51
    Duration                      msecond         201.08
    L1/TEX Cache Throughput             %          65.75
    L2 Cache Throughput                 %          22.96
    SM Active Cycles                cycle 196,069,739.10
    Compute (SM) Throughput             %          34.09
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                819,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.85
    Achieved Active Warps Per SM           warp        31.63
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 8 16 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 8x16
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 9798 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5120.14 ms
==PROF== Disconnected from process 9798
[9798] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (128, 64, 50)x(8, 16, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         975.30
    Elapsed Cycles                  cycle    199,746,726
    Memory Throughput                   %          32.18
    DRAM Throughput                     %          20.94
    Duration                      msecond         204.81
    L1/TEX Cache Throughput             %          64.36
    L2 Cache Throughput                 %          22.30
    SM Active Cycles                cycle 199,730,049.43
    Compute (SM) Throughput             %          33.46
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.63
    Achieved Active Warps Per SM           warp        31.24
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 8 32 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 8x32
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 9903 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5074.13 ms
==PROF== Disconnected from process 9903
[9903] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (128, 32, 50)x(8, 32, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.96
    Elapsed Cycles                  cycle    198,849,240
    Memory Throughput                   %          32.13
    DRAM Throughput                     %          20.67
    Duration                      msecond         203.96
    L1/TEX Cache Throughput             %          64.27
    L2 Cache Throughput                 %          21.98
    SM Active Cycles                cycle 198,862,577.73
    Compute (SM) Throughput             %          33.61
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.14
    Achieved Active Warps Per SM           warp        30.44
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 8 64 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 8x64
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10001 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5065.76 ms
==PROF== Disconnected from process 10001
[10001] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (128, 16, 50)x(8, 64, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.69
    Elapsed Cycles                  cycle    195,630,748
    Memory Throughput                   %          32.41
    DRAM Throughput                     %          20.78
    Duration                      msecond         200.71
    L1/TEX Cache Throughput             %          64.81
    L2 Cache Throughput                 %          22.25
    SM Active Cycles                cycle 195,613,546.97
    Compute (SM) Throughput             %          34.16
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.46
    Achieved Active Warps Per SM           warp        29.27
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 8 128 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 8x128
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10088 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5014.25 ms
==PROF== Disconnected from process 10088
[10088] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (128, 8, 50)x(8, 128, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.90
    Elapsed Cycles                  cycle    194,560,394
    Memory Throughput                   %          32.04
    DRAM Throughput                     %          22.32
    Duration                      msecond         199.57
    L1/TEX Cache Throughput             %          64.08
    L2 Cache Throughput                 %          22.55
    SM Active Cycles                cycle 194,274,861.27
    Compute (SM) Throughput             %          34.35
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.81
    Achieved Active Warps Per SM           warp        28.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 16 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 16x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10154 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 8227.4 ms
==PROF== Disconnected from process 10154
[10154] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (64, 1023, 50)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         973.20
    Elapsed Cycles                  cycle    588,586,078
    Memory Throughput                   %          14.97
    DRAM Throughput                     %           6.81
    Duration                      msecond         603.71
    L1/TEX Cache Throughput             %          16.86
    L2 Cache Throughput                 %           8.04
    SM Active Cycles                cycle 587,495,422.97
    Compute (SM) Throughput             %          22.73
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              3,273,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               6,820
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.89
    Achieved Active Warps Per SM           warp        15.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 16 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 16x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10220 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 6117.89 ms
==PROF== Disconnected from process 10220
[10220] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (64, 512, 50)x(16, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         972.24
    Elapsed Cycles                  cycle    328,924,221
    Memory Throughput                   %          13.42
    DRAM Throughput                     %          12.19
    Duration                      msecond         337.36
    L1/TEX Cache Throughput             %          23.29
    L2 Cache Throughput                 %          11.49
    SM Active Cycles                cycle 327,892,008.80
    Compute (SM) Throughput             %          20.38
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              1,638,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            3,413.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.90
    Achieved Active Warps Per SM           warp        15.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 16 4 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 16x4
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10286 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5033.74 ms
==PROF== Disconnected from process 10286
[10286] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (64, 256, 50)x(16, 4, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.88
    Elapsed Cycles                  cycle    196,464,082
    Memory Throughput                   %          22.41
    DRAM Throughput                     %          20.43
    Duration                      msecond         201.53
    L1/TEX Cache Throughput             %          38.75
    L2 Cache Throughput                 %          16.79
    SM Active Cycles                cycle 196,529,567.80
    Compute (SM) Throughput             %          34.02
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                819,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.64
    Achieved Active Warps Per SM           warp        31.57
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 16 8 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 16x8
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10352 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5076.23 ms
==PROF== Disconnected from process 10352
[10352] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (64, 128, 50)x(16, 8, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.99
    Elapsed Cycles                  cycle    197,783,795
    Memory Throughput                   %          22.26
    DRAM Throughput                     %          20.31
    Duration                      msecond         202.86
    L1/TEX Cache Throughput             %          38.47
    L2 Cache Throughput                 %          15.59
    SM Active Cycles                cycle 197,778,374.37
    Compute (SM) Throughput             %          33.79
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.68
    Achieved Active Warps Per SM           warp        31.26
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 16 16 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 16x16
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10418 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5363.52 ms
==PROF== Disconnected from process 10418
[10418] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (64, 64, 50)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         969.68
    Elapsed Cycles                  cycle    201,917,284
    Memory Throughput                   %          21.91
    DRAM Throughput                     %          20.62
    Duration                      msecond         207.15
    L1/TEX Cache Throughput             %          37.81
    L2 Cache Throughput                 %          14.51
    SM Active Cycles                cycle 200,856,605.40
    Compute (SM) Throughput             %          33.27
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.21
    Achieved Active Warps Per SM           warp        30.47
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 16 32 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 16x32
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10484 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5276.4 ms
==PROF== Disconnected from process 10484
[10484] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (64, 32, 50)x(16, 32, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         969.62
    Elapsed Cycles                  cycle    202,493,887
    Memory Throughput                   %          21.86
    DRAM Throughput                     %          20.29
    Duration                      msecond         207.70
    L1/TEX Cache Throughput             %          37.61
    L2 Cache Throughput                 %          14.14
    SM Active Cycles                cycle 201,345,674.53
    Compute (SM) Throughput             %          33.19
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        90.87
    Achieved Active Warps Per SM           warp        29.08
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 16 64 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 16x64
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10550 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5218.21 ms
==PROF== Disconnected from process 10550
[10550] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (64, 16, 50)x(16, 64, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         970.56
    Elapsed Cycles                  cycle    197,060,727
    Memory Throughput                   %          22.44
    DRAM Throughput                     %          20.80
    Duration                      msecond         202.09
    L1/TEX Cache Throughput             %          38.40
    L2 Cache Throughput                 %          14.42
    SM Active Cycles                cycle 195,556,992.13
    Compute (SM) Throughput             %          34.07
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        85.70
    Achieved Active Warps Per SM           warp        27.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 14.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 32 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 32x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10616 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 6129.54 ms
==PROF== Disconnected from process 10616
[10616] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (32, 1023, 50)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         972.21
    Elapsed Cycles                  cycle    310,474,532
    Memory Throughput                   %          14.20
    DRAM Throughput                     %          12.92
    Duration                      msecond         318.53
    L1/TEX Cache Throughput             %          17.78
    L2 Cache Throughput                 %          12.81
    SM Active Cycles                cycle 309,591,892.67
    Compute (SM) Throughput             %          21.56
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                              1,636,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               3,410
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.90
    Achieved Active Warps Per SM           warp        15.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 32 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 32x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10682 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5029.29 ms
==PROF== Disconnected from process 10682
[10682] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (32, 512, 50)x(32, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.75
    Elapsed Cycles                  cycle    180,634,775
    Memory Throughput                   %          24.35
    DRAM Throughput                     %          22.22
    Duration                      msecond         185.31
    L1/TEX Cache Throughput             %          30.42
    L2 Cache Throughput                 %          17.14
    SM Active Cycles                cycle 180,660,075.73
    Compute (SM) Throughput             %          36.96
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                819,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.52
    Achieved Active Warps Per SM           warp        31.53
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 32 4 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 32x4
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10748 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5054.03 ms
==PROF== Disconnected from process 10748
[10748] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (32, 256, 50)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.89
    Elapsed Cycles                  cycle    179,911,732
    Memory Throughput                   %          24.44
    DRAM Throughput                     %          22.31
    Duration                      msecond         184.54
    L1/TEX Cache Throughput             %          30.48
    L2 Cache Throughput                 %          15.18
    SM Active Cycles                cycle 179,898,616.60
    Compute (SM) Throughput             %          37.11
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.19
    Achieved Active Warps Per SM           warp        31.42
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 32 8 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 32x8
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10814 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5109.73 ms
==PROF== Disconnected from process 10814
[10814] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (32, 128, 50)x(32, 8, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.94
    Elapsed Cycles                  cycle    183,752,249
    Memory Throughput                   %          23.93
    DRAM Throughput                     %          21.86
    Duration                      msecond         188.48
    L1/TEX Cache Throughput             %          29.76
    L2 Cache Throughput                 %          13.71
    SM Active Cycles                cycle 183,717,003.10
    Compute (SM) Throughput             %          36.34
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.24
    Achieved Active Warps Per SM           warp        30.80
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 32 16 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 32x16
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10880 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5212.5 ms
==PROF== Disconnected from process 10880
[10880] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (32, 64, 50)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         969.44
    Elapsed Cycles                  cycle    188,066,196
    Memory Throughput                   %          23.51
    DRAM Throughput                     %          21.96
    Duration                      msecond         192.93
    L1/TEX Cache Throughput             %          29.01
    L2 Cache Throughput                 %          12.82
    SM Active Cycles                cycle 187,026,885.87
    Compute (SM) Throughput             %          35.70
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        92.56
    Achieved Active Warps Per SM           warp        29.62
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 32 32 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 32x32
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 10972 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5164.99 ms
==PROF== Disconnected from process 10972
[10972] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (32, 32, 50)x(32, 32, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         969.56
    Elapsed Cycles                  cycle    189,325,137
    Memory Throughput                   %          23.35
    DRAM Throughput                     %          21.73
    Duration                      msecond         194.21
    L1/TEX Cache Throughput             %          28.25
    L2 Cache Throughput                 %          12.47
    SM Active Cycles                cycle 187,986,512.17
    Compute (SM) Throughput             %          35.46
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.15
    Achieved Active Warps Per SM           warp        27.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 64 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 64x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 11135 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5177.5 ms
==PROF== Disconnected from process 11135
[11135] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (16, 1023, 50)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         975.81
    Elapsed Cycles                  cycle    183,124,157
    Memory Throughput                   %          24.01
    DRAM Throughput                     %          21.93
    Duration                      msecond         187.66
    L1/TEX Cache Throughput             %          29.88
    L2 Cache Throughput                 %          19.71
    SM Active Cycles                cycle 183,275,132.17
    Compute (SM) Throughput             %          36.46
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                818,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.18
    Achieved Active Warps Per SM           warp        31.42
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 64 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 64x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 11296 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5092.98 ms
==PROF== Disconnected from process 11296
[11296] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (16, 512, 50)x(64, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.70
    Elapsed Cycles                  cycle    178,837,312
    Memory Throughput                   %          24.59
    DRAM Throughput                     %          22.44
    Duration                      msecond         183.48
    L1/TEX Cache Throughput             %          30.52
    L2 Cache Throughput                 %          16.17
    SM Active Cycles                cycle 178,827,964.60
    Compute (SM) Throughput             %          37.33
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.89
    Achieved Active Warps Per SM           warp        31.65
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 64 4 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 64x4
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 11447 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5105.64 ms
==PROF== Disconnected from process 11447
[11447] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (16, 256, 50)x(64, 4, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         968.86
    Elapsed Cycles                  cycle    179,154,396
    Memory Throughput                   %          24.70
    DRAM Throughput                     %          22.39
    Duration                      msecond         183.76
    L1/TEX Cache Throughput             %          30.53
    L2 Cache Throughput                 %          14.00
    SM Active Cycles                cycle 178,119,520.07
    Compute (SM) Throughput             %          37.50
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.91
    Achieved Active Warps Per SM           warp        31.33
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 64 8 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 64x8
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 11532 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5063.38 ms
==PROF== Disconnected from process 11532
[11532] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (16, 128, 50)x(64, 8, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         969.27
    Elapsed Cycles                  cycle    181,840,064
    Memory Throughput                   %          24.31
    DRAM Throughput                     %          22.08
    Duration                      msecond         186.62
    L1/TEX Cache Throughput             %          29.96
    L2 Cache Throughput                 %          12.75
    SM Active Cycles                cycle 180,855,055.10
    Compute (SM) Throughput             %          36.91
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.05
    Achieved Active Warps Per SM           warp        30.73
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 64 16 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 64x16
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 11624 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5147.54 ms
==PROF== Disconnected from process 11624
[11624] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (16, 64, 50)x(64, 16, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.83
    Elapsed Cycles                  cycle    186,065,350
    Memory Throughput                   %          23.63
    DRAM Throughput                     %          22.21
    Duration                      msecond         190.87
    L1/TEX Cache Throughput             %          28.92
    L2 Cache Throughput                 %          11.95
    SM Active Cycles                cycle 185,754,183.30
    Compute (SM) Throughput             %          35.88
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        92.86
    Achieved Active Warps Per SM           warp        29.71
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 128 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 128x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 11721 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5052.72 ms
==PROF== Disconnected from process 11721
[11721] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (8, 1023, 50)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.63
    Elapsed Cycles                  cycle    179,724,317
    Memory Throughput                   %          24.47
    DRAM Throughput                     %          22.34
    Duration                      msecond         184.40
    L1/TEX Cache Throughput             %          30.09
    L2 Cache Throughput                 %          19.42
    SM Active Cycles                cycle 179,733,821.83
    Compute (SM) Throughput             %          37.15
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                409,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.70
    Achieved Active Warps Per SM           warp        31.58
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 128 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 128x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 11813 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5006.12 ms
==PROF== Disconnected from process 11813
[11813] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (8, 512, 50)x(128, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         975.10
    Elapsed Cycles                  cycle    176,640,766
    Memory Throughput                   %          24.90
    DRAM Throughput                     %          22.72
    Duration                      msecond         181.15
    L1/TEX Cache Throughput             %          30.45
    L2 Cache Throughput                 %          15.64
    SM Active Cycles                cycle 176,622,575.03
    Compute (SM) Throughput             %          37.80
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,800
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.32
    Achieved Active Warps Per SM           warp        31.46
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 128 4 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 128x4
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 11900 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 5029.14 ms
==PROF== Disconnected from process 11900
[11900] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (8, 256, 50)x(128, 4, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         975.04
    Elapsed Cycles                  cycle    176,068,276
    Memory Throughput                   %          24.98
    DRAM Throughput                     %          22.79
    Duration                      msecond         180.58
    L1/TEX Cache Throughput             %          30.38
    L2 Cache Throughput                 %          13.65
    SM Active Cycles                cycle 176,064,937.67
    Compute (SM) Throughput             %          37.92
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.38
    Achieved Active Warps Per SM           warp        31.16
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 128 8 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 128x8
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 11987 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 4991.07 ms
==PROF== Disconnected from process 11987
[11987] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (8, 128, 50)x(128, 8, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.80
    SM Frequency            cycle/usecond         969.34
    Elapsed Cycles                  cycle    177,853,437
    Memory Throughput                   %          24.87
    DRAM Throughput                     %          22.57
    Duration                      msecond         182.45
    L1/TEX Cache Throughput             %          30.16
    L2 Cache Throughput                 %          12.49
    SM Active Cycles                cycle 176,482,201.47
    Compute (SM) Throughput             %          37.75
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.93
    Achieved Active Warps Per SM           warp        30.70
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 256 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 256x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 12074 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 4926.92 ms
==PROF== Disconnected from process 12074
[12074] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (4, 1023, 50)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         968.10
    Elapsed Cycles                  cycle    172,312,775
    Memory Throughput                   %          25.68
    DRAM Throughput                     %          23.28
    Duration                      msecond         176.87
    L1/TEX Cache Throughput             %          30.72
    L2 Cache Throughput                 %          19.76
    SM Active Cycles                cycle 171,278,424.93
    Compute (SM) Throughput             %          38.99
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                204,600
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.60
    Achieved Active Warps Per SM           warp        31.23
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 256 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 256x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 12162 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 4948.19 ms
==PROF== Disconnected from process 12162
[12162] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (4, 512, 50)x(256, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.88
    Elapsed Cycles                  cycle    169,430,429
    Memory Throughput                   %          25.96
    DRAM Throughput                     %          23.68
    Duration                      msecond         173.80
    L1/TEX Cache Throughput             %          30.87
    L2 Cache Throughput                 %          15.93
    SM Active Cycles                cycle 169,452,115.20
    Compute (SM) Throughput             %          39.41
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,400
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.35
    Achieved Active Warps Per SM           warp        31.15
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 256 4 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 256x4
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 12259 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 4922.42 ms
==PROF== Disconnected from process 12259
[12259] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (4, 256, 50)x(256, 4, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.38
    Elapsed Cycles                  cycle    168,533,717
    Memory Throughput                   %          26.09
    DRAM Throughput                     %          23.80
    Duration                      msecond         172.96
    L1/TEX Cache Throughput             %          30.84
    L2 Cache Throughput                 %          13.95
    SM Active Cycles                cycle 168,248,034.90
    Compute (SM) Throughput             %          39.62
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.91
    Achieved Active Warps Per SM           warp        31.01
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 512 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 512x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 12351 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 4826.45 ms
==PROF== Disconnected from process 12351
[12351] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (2, 1023, 50)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.98
    Elapsed Cycles                  cycle    156,442,709
    Memory Throughput                   %          28.11
    DRAM Throughput                     %          25.65
    Duration                      msecond         160.46
    L1/TEX Cache Throughput             %          31.95
    L2 Cache Throughput                 %          21.47
    SM Active Cycles                cycle 156,401,911.43
    Compute (SM) Throughput             %          42.68
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                102,300
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.37
    Achieved Active Warps Per SM           warp        30.84
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 512 2 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 512x2
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 12438 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 4898.93 ms
==PROF== Disconnected from process 12438
[12438] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (2, 512, 50)x(512, 2, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.71
    Elapsed Cycles                  cycle    160,542,809
    Memory Throughput                   %          27.39
    DRAM Throughput                     %          24.98
    Duration                      msecond         164.71
    L1/TEX Cache Throughput             %          31.12
    L2 Cache Throughput                 %          16.69
    SM Active Cycles                cycle 160,203,954.90
    Compute (SM) Throughput             %          41.59
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,200
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,428,800
    Waves Per SM                                            1,706.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.07
    Achieved Active Warps Per SM           warp        30.74
    ------------------------------- ----------- ------------


====================================================================================================
Running Nsight Compute for kernel: convolution2DKernel in directory: ./conv/build with args: 1024 1 1 3 1024 1024 50 2 2 16
Command-line arguments:
CUDA block dimensions: 1024x1
Initialization type: 1
Input dimensions: 16x3x1024x1024
Filter dimensions: 50x3x2x2
CPU cores: max available
==PROF== Connected to process 12525 (/home/tesla/exp/Notes/Cuda_kernels/conv/build/convolution)
Input shape: [16, 3, 1024, 1024]
Filter shape: [50, 3, 2, 2]
==PROF== Profiling "convolution2DKernel": 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 8 passes
GPU Output shape: [16, 50, 1023, 1023]
GPU Convolution Time: 4658.76 ms
==PROF== Disconnected from process 12525
[12525] convolution@127.0.0.1
  convolution2DKernel(float *, float *, float *, int, int, int, int, int, int, int, int, int) (1, 1023, 50)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- --------------
    Metric Name               Metric Unit   Metric Value
    ----------------------- ------------- --------------
    DRAM Frequency          cycle/nsecond           6.79
    SM Frequency            cycle/usecond         974.47
    Elapsed Cycles                  cycle    141,436,860
    Memory Throughput                   %          31.09
    DRAM Throughput                     %          28.85
    Duration                      msecond         145.14
    L1/TEX Cache Throughput             %          32.95
    L2 Cache Throughput                 %          23.73
    SM Active Cycles                cycle 141,257,229.03
    Compute (SM) Throughput             %          47.21
    ----------------------- ------------- --------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                 1,024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 51,150
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread      52,377,600
    Waves Per SM                                               1,705
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.60
    Achieved Active Warps Per SM           warp        30.91
    ------------------------------- ----------- ------------


====================================================================================================
