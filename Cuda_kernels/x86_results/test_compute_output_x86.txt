   buildDirectory               kernelName  blockSizeX  blockSizeY  totalThreads smFrequencyCyclePerUsecond  elapsedCycles memoryThroughputPercent dramThroughputPercent  durationUsecond l1TexCacheThroughputPercent l2CacheThroughputPercent  smActiveCycles computeSMThroughputPercent  blockSize  gridSize  registersPerThread sharedMemoryConfigSizeKbyte  driverSharedMemoryPerBlockByte dynamicSharedMemoryPerBlockKbyte  staticSharedMemoryPerBlockByte wavesPerSM  blockLimitSm  blockLimitRegisters  blockLimitSharedMem  blockLimitWarps  theoreticalActiveWarpsPerSm theoreticalOccupancyPercent achievedOccupancyPercent achievedActiveWarpsPerSm warningsWrn                                                                                                                                                                                                                                                                                                                                                           informationInf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           optimizationHintsOpt
./attention/build           compute_output           1           1             1                 960.530000      174579828               82.130000              7.260000         181750.0                   82.130000                22.980000       174602093                  82.130000          1   1048576                  49                    0.032002                               0                         0.000000                               0   2.000000            16                   36                   16               32                           16                   50.000000                49.880000                15.960000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                   OPT   Est. Speedup: 96.88%                                                                                          \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           1           2             2                 960.340000      110572668               64.830000              5.740000         115140.0                   97.240000                18.290000       110578249                  64.830000          2    524288                  49                    0.032002                               0                         0.000000                               0   1.000000            16                   36                   16               32                           16                   50.000000                49.900000                15.970000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                          OPT   Est. Speedup: 93.75%                                                                                          \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 2      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           1           4             4                 960.070000       89671603               49.950000              3.550000          93400.0                   99.900000                12.320000        89660373                  39.970000          4    262144                  49                    0.032002                               0                         0.000000                               0 546.130000            16                   36                   16               32                           16                   50.000000                49.920000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 87.5%                                                                                           \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 4      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           1           8             8                 959.990000       80749914               49.930000              1.990000          84110.0                   99.860000                 5.020000        80707828                  22.190000          8    131072                  49                    0.032002                               0                         0.000000                               0 273.070000            16                   36                   16               32                           16                   50.000000                49.920000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 75%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           1          16            16                 959.840000       76232839               49.940000              1.070000          79420.0                   99.890000                 3.760000        76172870                  11.750000         16     65536                  49                    0.032002                               0                         0.000000                               0 136.530000            16                   36                   16               32                           16                   50.000000                49.870000                15.960000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           1          32            32                 960.440000       74041298               49.900000              0.570000          77090.0                   99.810000                 3.130000        73896991                   6.050000         32     32768                  49                    0.032002                               0                         0.000000                               0  68.270000            16                   36                   16               32                           16                   50.000000                49.760000                15.920000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           1          64            64                 959.850000       74157928               49.840000              0.300000          77260.0                   99.670000                 6.260000        74000438                   6.040000         64     16384                  49                    0.032002                               0                         0.000000                               0  34.130000            16                   18                   16               16                           32                  100.000000                97.380000                31.160000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           1         128           128                 960.460000       74489537               49.620000              0.170000          77550.0                   99.240000                13.160000        74327068                   6.020000        128      8192                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    9                   16                8                           32                  100.000000                98.790000                31.610000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           1         256           256                 960.080000       74519377               49.600000              0.100000          77610.0                   99.200000                13.950000        74176327                   6.010000        256      4096                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    4                   16                4                           32                  100.000000                98.710000                31.590000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           1         512           512                 959.840000       74548399               49.630000              0.080000          77660.0                   99.270000                20.590000        74123611                   6.010000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                98.820000                31.620000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           1        1024          1024                 959.200000       75427105               56.660000              0.620000          78580.0                   98.550000                56.660000        74276466                   5.940000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.430000                31.500000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           2           1             2                 959.170000       88308819               81.290000             14.350000          91950.0                   81.290000                26.190000        88200733                  81.290000          2    524288                  49                    0.032002                               0                         0.000000                               0   1.000000            16                   36                   16               32                           16                   50.000000                49.880000                15.960000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                   OPT   Est. Speedup: 93.75%                                                                                          \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 2      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           2           2             4                 959.490000       55077514               65.120000             11.520000          57370.0                   97.670000                21.300000        55068056                  65.120000          4    262144                  49                    0.032002                               0                         0.000000                               0 546.130000            16                   36                   16               32                           16                   50.000000                49.890000                15.960000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                          OPT   Est. Speedup: 87.5%                                                                                           \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 4      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           2           4             8                 959.710000       44841623               49.970000              7.100000          46710.0                   99.930000                13.960000        44811954                  39.980000          8    131072                  49                    0.032002                               0                         0.000000                               0 273.070000            16                   36                   16               32                           16                   50.000000                49.900000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 75%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           2           8            16                 936.310000       40360966               49.950000              4.030000          43100.0                   99.900000                 7.050000        40324240                  22.200000         16     65536                  49                    0.032002                               0                         0.000000                               0 136.530000            16                   36                   16               32                           16                   50.000000                49.860000                15.960000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           2          16            32                 937.890000       38141947               49.910000              2.170000          40660.0                   99.820000                 4.590000        38070016                  11.750000         32     32768                  49                    0.032002                               0                         0.000000                               0  68.270000            16                   36                   16               32                           16                   50.000000                49.750000                15.920000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           2          32            64                 938.780000       38172324               49.880000              1.120000          40660.0                   99.750000                 6.040000        38078193                  11.740000         64     16384                  49                    0.032002                               0                         0.000000                               0  34.130000            16                   18                   16               16                           32                  100.000000                97.040000                31.050000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           2          64           128                 938.140000       38254815               49.770000              0.590000          40780.0                   99.540000                 9.740000        38176900                  11.710000        128      8192                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    9                   16                8                           32                  100.000000                98.850000                31.630000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           2         128           256                 937.370000       38296150               49.720000              0.330000          40850.0                   99.430000                 9.480000        38128264                  11.700000        256      4096                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    4                   16                4                           32                  100.000000                98.740000                31.600000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           2         256           512                 943.940000       38564841               49.360000              0.200000          40850.0                   98.730000                 9.120000        38123481                  11.620000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                98.820000                31.620000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           2         512          1024                 944.340000       38606484               49.420000              0.170000          40880.0                   98.830000                21.270000        38087652                  11.610000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.480000                31.510000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           4           1             4                 933.290000       45234537               79.240000             28.370000          48470.0                   79.250000                26.470000        45228598                  79.240000          4    262144                  49                    0.032002                               0                         0.000000                               0 546.130000            16                   36                   16               32                           16                   50.000000                49.870000                15.960000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                          OPT   Est. Speedup: 87.5%                                                                                           \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 4      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           4           2             8                 931.430000       27740269               64.700000             23.150000          29740.0                   97.030000                22.030000        27684056                  64.700000          8    131072                  49                    0.032002                               0                         0.000000                               0 273.070000            16                   36                   16               32                           16                   50.000000                49.870000                15.960000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                          OPT   Est. Speedup: 75%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           4           4            16                 932.570000       22429159               49.960000             14.340000          24040.0                   99.920000                15.030000        22405560                  39.980000         16     65536                  49                    0.032002                               0                         0.000000                               0 136.530000            16                   36                   16               32                           16                   50.000000                49.850000                15.950000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           4           8            32                 936.030000       20199327               49.910000              7.900000          21580.0                   99.820000                 9.040000        20161010                  22.190000         32     32768                  49                    0.032002                               0                         0.000000                               0  68.270000            16                   36                   16               32                           16                   50.000000                49.750000                15.920000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           4          16            64                 935.590000       20214573               49.870000              4.010000          21600.0                   99.740000                 7.810000        20167707                  22.170000         64     16384                  49                    0.032002                               0                         0.000000                               0  34.130000            16                   18                   16               16                           32                  100.000000                97.300000                31.140000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           4          32           128                 936.170000       20250642               49.790000              2.090000          21630.0                   99.580000                 9.430000        20209349                  22.130000        128      8192                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    9                   16                8                           32                  100.000000                98.880000                31.640000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           4          64           256                 936.680000       20252698               49.780000              1.110000          21620.0                   99.570000                 8.530000        20163831                  22.120000        256      4096                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    4                   16                4                           32                  100.000000                98.780000                31.610000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           4         128           512                 943.430000       20412967               49.390000              0.610000          21640.0                   98.770000                 8.120000        20178590                  21.950000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                98.790000                31.610000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           4         256          1024                 944.220000       20402702               49.400000              0.360000          21610.0                   98.800000                 8.070000        20125617                  21.960000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.190000                31.420000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           8           1             8                 924.400000       23116722               77.530000             54.530000          25010.0                   77.680000                26.570000        23070103                  77.530000          8    131072                  49                    0.032002                               0                         0.000000                               0 273.070000            16                   36                   16               32                           16                   50.000000                49.850000                15.950000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                          OPT   Est. Speedup: 75%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           8           2            16                 924.760000       14091521               63.590000             44.030000          15240.0                   95.380000                22.970000        14078172                  63.590000         16     65536                  49                    0.032002                               0                         0.000000                               0 136.530000            16                   36                   16               32                           16                   50.000000                49.830000                15.950000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                          OPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           8           4            32                 930.990000       11216917               49.920000             27.690000          12050.0                   99.840000                15.630000        11206734                  39.940000         32     32768                  49                    0.032002                               0                         0.000000                               0  68.270000            16                   36                   16               32                           16                   50.000000                49.770000                15.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output           8           8            64                 930.580000       11237371               49.830000             13.880000          12080.0                   99.660000                12.600000        11212240                  39.870000         64     16384                  49                    0.032002                               0                         0.000000                               0  34.130000            16                   18                   16               16                           32                  100.000000                96.390000                30.840000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           8          16           128                 931.260000       11304501               49.530000              7.190000          12140.0                   99.070000                 9.960000        11290481                  39.630000        128      8192                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    9                   16                8                           32                  100.000000                98.910000                31.650000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           8          32           256                 933.450000       11330700               49.420000              3.700000          12140.0                   98.840000                 8.260000        11306344                  39.540000        256      4096                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    4                   16                4                           32                  100.000000                98.910000                31.650000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           8          64           512                 939.570000       11337984               49.390000              1.960000          12070.0                   98.780000                 7.500000        11213297                  39.520000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                98.860000                31.630000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output           8         128          1024                 942.020000       11352209               49.330000              1.080000          12050.0                   98.660000                 6.930000        11189672                  39.470000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.540000                31.530000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build           compute_output          16           1            16                 889.370000       12304320               72.830000             70.370000          13830.0                   72.840000                49.140000        12675242                  72.830000         16     65536                  49                    0.032002                               0                         0.000000                               0 136.530000            16                   36                   16               32                           16                   50.000000                49.830000                15.940000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                          OPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output          16           2            32                 920.930000        7264231               69.350000             69.350000           7860.0                   92.880000                43.240000         7198152                  61.910000         32     32768                  49                    0.032002                               0                         0.000000                               0  68.270000            16                   36                   16               32                           16                   50.000000                49.760000                15.920000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                OPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output          16           4            64                 921.330000        7125441               62.990000             40.190000           7720.0                   94.490000                30.650000         7100862                  62.990000         64     16384                  49                    0.032002                               0                         0.000000                               0  34.130000            16                   18                   16               16                           32                  100.000000                96.930000                31.020000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          16           8           128                 923.870000        7272401               62.210000             20.960000           7800.0                   93.330000                16.250000         7192224                  62.210000        128      8192                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    9                   16                8                           32                  100.000000                98.960000                31.670000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          16          16           256                 925.700000        7190010               62.410000             11.210000           7760.0                   93.630000                10.400000         7152765                  62.410000        256      4096                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    4                   16                4                           32                  100.000000                98.960000                31.670000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          16          32           512                 928.440000        7445346               60.630000              5.480000           7960.0                   90.970000                 7.840000         7336780                  60.630000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                98.930000                31.660000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          16          64          1024                 927.980000        7043239               64.130000              3.070000           7530.0                   96.220000                 6.840000         6855162                  64.130000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.750000                31.600000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          32           1            32                 917.960000        7843691               82.650000             82.650000           8540.0                   58.630000                76.470000         7778616                  57.170000         32     32768                  49                    0.032002                               0                         0.000000                               0  68.270000            16                   36                   16               32                           16                   50.000000                49.790000                15.930000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing DRAM in the Memory Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               OPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build           compute_output          32           2            64                 911.310000        5833898               78.710000             60.020000           6280.0                   78.360000                78.710000         5717783                  78.230000         64     16384                  49                    0.032002                               0                         0.000000                               0  34.130000            16                   18                   16               16                           32                  100.000000                97.120000                31.080000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          32           4           128                 918.210000        5799653               78.350000             48.420000           6230.0                   78.510000                30.840000         5707224                  78.350000        128      8192                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    9                   16                8                           32                  100.000000                98.980000                31.670000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          32           8           256                 920.640000        5761683               78.620000             26.260000           6190.0                   78.860000                16.700000         5681647                  78.620000        256      4096                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    4                   16                4                           32                  100.000000                98.970000                31.670000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          32          16           512                 923.640000        5875727               76.980000             13.140000           6300.0                   77.330000                 9.800000         5793614                  76.980000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.050000                31.700000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          32          32          1024                 923.460000        5423982               83.390000              7.560000           5820.0                   84.880000                 7.120000         5278660                  83.390000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.970000                31.670000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    N/A
./attention/build           compute_output          64           1            64                 910.110000        6704712               90.560000             69.530000           7280.0                   67.740000                90.560000         6646700                  67.600000         64     16384                  49                    0.032002                               0                         0.000000                               0  34.130000            16                   18                   16               16                           32                  100.000000                97.430000                31.180000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing L2 in the Memory Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N/A
./attention/build           compute_output          64           2           128                 914.390000        5806287               78.440000             57.230000           6250.0                   78.670000                64.450000         5695196                  78.440000        128      8192                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    9                   16                8                           32                  100.000000                98.650000                31.570000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           N/A
./attention/build           compute_output          64           4           256                 918.220000        5606178               80.960000             30.170000           6030.0                   81.000000                29.520000         5553901                  80.960000        256      4096                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    4                   16                4                           32                  100.000000                98.630000                31.560000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    N/A
./attention/build           compute_output          64           8           512                 920.660000        5510082               82.180000             14.740000           5920.0                   82.870000                15.680000         5406549                  82.180000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                98.570000                31.540000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    N/A
./attention/build           compute_output          64          16          1024                 922.870000        5311093               85.260000              8.260000           5690.0                   87.400000                 9.060000         5126345                  85.260000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                99.040000                31.690000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    N/A
./attention/build           compute_output         128           1           128                 911.630000        6850611               89.280000             78.910000           7400.0                   67.050000                89.280000         6723581                  66.400000        128      8192                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    9                   16                8                           32                  100.000000                98.290000                31.450000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing L2 in the Memory Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N/A
./attention/build           compute_output         128           2           256                 913.160000        5586797               81.570000             30.050000           6020.0                   82.210000                61.690000         5450025                  81.570000        256      4096                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    4                   16                4                           32                  100.000000                98.420000                31.500000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    N/A
./attention/build           compute_output         128           4           512                 920.540000        5370737               84.390000             14.260000           5770.0                   85.560000                29.800000         5236514                  84.390000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                98.320000                31.460000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    N/A
./attention/build           compute_output         128           8          1024                 920.350000        5241954               86.560000              8.350000           5620.0                   88.720000                15.560000         5050225                  86.560000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                99.090000                31.710000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    N/A
./attention/build           compute_output         256           1           256                 909.930000        6775843               89.370000             69.490000           7360.0                   66.970000                89.370000         6710625                  66.920000        256      4096                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    4                   16                4                           32                  100.000000                97.100000                31.070000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing L2 in the Memory Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N/A
./attention/build           compute_output         256           2           512                 920.430000        5472153               83.050000             14.540000           5860.0                   84.040000                60.480000         5331252                  83.050000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                98.480000                31.510000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    N/A
./attention/build           compute_output         256           4          1024                 919.130000        5243447               86.630000              8.470000           5630.0                   88.810000                37.030000         5045135                  86.630000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                99.000000                31.680000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    N/A
./attention/build           compute_output         512           1           512                 913.780000        6777202               89.920000             54.750000           7310.0                   67.440000                89.920000         6646603                  67.070000        512      2048                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                97.090000                31.070000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing L2 in the Memory Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N/A
./attention/build           compute_output         512           2          1024                 916.290000        5681887               80.730000              9.780000           6100.0                   81.120000                80.730000         5523073                  80.120000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.740000                31.600000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing L2 in the Memory Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N/A
./attention/build           compute_output        1024           1          1024                 909.700000        6814954               89.300000             59.110000           7370.0                   67.420000                89.300000         6645489                  66.800000          1      1024                  49                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.620000                31.560000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing L2 in the Memory Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N/A
./attention/build compute_attention_scores           1           1             1                 972.500000      168766548               84.990000              7.630000         173480.0                   84.990000                 3.690000       168699673                  84.990000          1   1048576                  45                    0.032002                               0                         0.000000                               0   2.000000            16                   40                   16               32                           16                   50.000000                49.880000                15.960000         N/A INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n          Start by analyzing workloads in the Compute Workload Analysis section.                                                                                                                                                                                                                                                                                                                                                                                                                   OPT   Est. Speedup: 96.88%                                                                                          \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           1           2             2                 963.300000      108797006               65.900000              5.930000         112920.0                   98.840000                 3.000000       108766780                  65.900000          2    524288                  45                    0.032002                               0                         0.000000                               0   1.000000            16                   40                   16               32                           16                   50.000000                49.900000                15.970000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                          OPT   Est. Speedup: 93.75%                                                                                          \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 2      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           1           4             4                 956.400000       89630511               49.980000              3.610000          93710.0                   99.960000                 1.990000        89602472                  39.990000          4    262144                  45                    0.032002                               0                         0.000000                               0 546.130000            16                   40                   16               32                           16                   50.000000                49.920000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 87.5%                                                                                           \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 4      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           1           8             8                 957.520000       80659345               49.980000              2.010000          84230.0                   99.960000                 1.460000        80626854                  22.220000          8    131072                  45                    0.032002                               0                         0.000000                               0 273.070000            16                   40                   16               32                           16                   50.000000                49.910000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 75%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           1          16            16                 951.020000       76252302               49.960000              1.080000          80180.0                   99.920000                 2.380000        76187185                  11.750000         16     65536                  45                    0.032002                               0                         0.000000                               0 136.530000            16                   40                   16               32                           16                   50.000000                49.860000                15.950000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           1          32            32                 952.980000       73989906               49.940000              0.580000          77640.0                   99.880000                 2.350000        73894022                   6.060000         32     32768                  45                    0.032002                               0                         0.000000                               0  68.270000            16                   40                   16               32                           16                   50.000000                49.750000                15.920000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           1          64            64                 952.020000       74125370               49.850000              0.310000          77860.0                   99.710000                 5.670000        73968453                   6.040000         64     16384                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   20                   16               16                           32                  100.000000                97.490000                31.200000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           1         128           128                 948.920000       74508094               49.620000              0.170000          78500.0                   99.240000                13.980000        74362835                   6.020000        128      8192                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   10                   16                8                           32                  100.000000                98.760000                31.600000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           1         256           256                 951.030000       74622693               49.540000              0.100000          78450.0                   99.080000                14.480000        74293207                   6.010000        256      4096                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    5                   16                4                           32                  100.000000                98.890000                31.650000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           1         512           512                 946.440000       74659591               49.550000              0.080000          78860.0                   99.110000                17.590000        74204995                   6.000000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.160000                31.730000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           1        1024          1024                 963.600000       76537423               54.310000              0.610000          79340.0                   97.030000                54.310000        74508151                   5.860000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.830000                31.620000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           2           1             2                 965.890000      112913973               63.510000             11.390000         116860.0                   95.220000                 5.530000       112843812                  63.510000          2    524288                  45                    0.032002                               0                         0.000000                               0   1.000000            16                   40                   16               32                           16                   50.000000                49.900000                15.970000         N/A                                                                                                                           INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. \n          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                                                                                                                                                                                                                                                                                                                                                                                                          OPT   Est. Speedup: 93.75%                                                                                          \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 2      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           2           2             4                 950.730000       71944780               49.840000              8.940000          75650.0                   99.630000                 4.550000        71914843                  49.840000          4    262144                  45                    0.032002                               0                         0.000000                               0 546.130000            16                   40                   16               32                           16                   50.000000                49.910000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 87.5%                                                                                           \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 4      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           2           4             8                 942.050000       53771770               49.980000              5.920000          57070.0                   99.970000                 3.370000        53743354                  33.340000          8    131072                  45                    0.032002                               0                         0.000000                               0 273.070000            16                   40                   16               32                           16                   50.000000                49.900000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 75%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           2           8            16                 935.400000       44873112               49.970000              3.530000          47960.0                   99.930000                 2.860000        44832860                  19.970000         16     65536                  45                    0.032002                               0                         0.000000                               0 136.530000            16                   40                   16               32                           16                   50.000000                49.850000                15.950000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           2          16            32                 935.020000       40364706               49.940000              2.000000          43160.0                   99.890000                 2.850000        40309741                  11.100000         32     32768                  45                    0.032002                               0                         0.000000                               0  68.270000            16                   40                   16               32                           16                   50.000000                49.730000                15.910000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           2          32            64                 934.950000       40351275               49.950000              1.030000          43160.0                   99.900000                 4.420000        40288682                  11.100000         64     16384                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   20                   16               16                           32                  100.000000                97.280000                31.130000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           2          64           128                 936.560000       40407242               49.890000              0.560000          43140.0                   99.790000                 8.200000        40329592                  11.090000        128      8192                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   10                   16                8                           32                  100.000000                98.700000                31.580000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           2         128           256                 937.380000       40529532               49.740000              0.310000          43230.0                   99.480000                 8.550000        40340038                  11.060000        256      4096                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    5                   16                4                           32                  100.000000                98.860000                31.630000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           2         256           512                 942.990000       40824426               49.370000              0.180000          43290.0                   98.750000                 8.440000        40380218                  10.980000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.130000                31.720000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           2         512          1024                 942.960000       40830061               49.430000              0.140000          43290.0                   98.870000                16.510000        40282417                  10.980000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.780000                31.610000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           4           1             4                 959.380000       90367401               49.570000             13.970000          94140.0                   99.130000                 6.960000        90298522                  39.690000          4    262144                  45                    0.032002                               0                         0.000000                               0 546.130000            16                   40                   16               32                           16                   50.000000                49.920000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 87.5%                                                                                           \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 4      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           4           2             8                 935.550000       53814452               49.930000             11.570000          57500.0                   99.860000                 6.170000        53774255                  33.310000          8    131072                  45                    0.032002                               0                         0.000000                               0 273.070000            16                   40                   16               32                           16                   50.000000                49.910000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 75%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           4           4            16                 933.270000       35868953               49.960000              8.660000          38410.0                   99.920000                 5.160000        35819662                  25.000000         16     65536                  45                    0.032002                               0                         0.000000                               0 136.530000            16                   40                   16               32                           16                   50.000000                49.860000                15.960000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           4           8            32                 934.360000       26922560               49.920000              5.860000          28810.0                   99.850000                 4.290000        26874166                  16.650000         32     32768                  45                    0.032002                               0                         0.000000                               0  68.270000            16                   40                   16               32                           16                   50.000000                49.720000                15.910000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           4          16            64                 934.790000       26925928               49.910000              2.900000          28800.0                   99.820000                 4.740000        26866365                  16.640000         64     16384                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   20                   16               16                           32                  100.000000                97.290000                31.130000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           4          32           128                 935.350000       26928816               49.910000              1.540000          28780.0                   99.820000                 6.270000        26872416                  16.640000        128      8192                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   10                   16                8                           32                  100.000000                98.660000                31.570000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           4          64           256                 936.720000       26973510               49.820000              0.830000          28790.0                   99.650000                 6.000000        26851410                  16.610000        256      4096                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    5                   16                4                           32                  100.000000                98.810000                31.620000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           4         128           512                 943.650000       27185200               49.430000              0.460000          28800.0                   98.850000                 6.100000        26882027                  16.480000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.100000                31.710000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           4         256          1024                 945.150000       27557121               48.760000              0.270000          29150.0                   97.530000                 5.680000        26817790                  16.260000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.650000                31.570000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           8           1             8                 955.380000       80759654               49.900000             14.900000          84510.0                   99.810000                 7.730000        80711556                  22.200000          8    131072                  45                    0.032002                               0                         0.000000                               0 273.070000            16                   40                   16               32                           16                   50.000000                49.920000                15.970000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 75%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           8           2            16                 933.620000       44817727               49.960000             13.170000          47990.0                   99.920000                 7.240000        44776046                  20.000000         16     65536                  45                    0.032002                               0                         0.000000                               0 136.530000            16                   40                   16               32                           16                   50.000000                49.860000                15.950000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           8           4            32                 934.680000       26904156               49.940000             11.020000          28780.0                   99.870000                 6.460000        26864742                  16.660000         32     32768                  45                    0.032002                               0                         0.000000                               0  68.270000            16                   40                   16               32                           16                   50.000000                49.740000                15.920000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores           8           8            64                 935.380000       26920041               49.900000              4.700000          28780.0                   99.800000                 5.510000        26862077                  16.650000         64     16384                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   20                   16               16                           32                  100.000000                97.070000                31.060000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           8          16           128                 935.650000       26919588               49.910000              2.970000          28760.0                   99.810000                 4.160000        26858525                  16.650000        128      8192                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   10                   16                8                           32                  100.000000                98.650000                31.570000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           8          32           256                 936.710000       26954482               49.830000              1.540000          28770.0                   99.670000                 3.450000        26817428                  16.630000        256      4096                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    5                   16                4                           32                  100.000000                98.880000                31.640000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           8          64           512                 943.330000       27150014               49.470000              0.830000          28780.0                   98.940000                 3.100000        26807760                  16.510000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.170000                31.730000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores           8         128          1024                 944.390000       27559317               48.740000              0.450000          29180.0                   97.470000                 2.890000        26843498                  16.260000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.690000                31.580000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          16           1            16                 952.700000       76192726               49.940000              8.220000          79960.0                   99.880000                 8.740000        76142365                  11.760000         16     65536                  45                    0.032002                               0                         0.000000                               0 136.530000            16                   40                   16               32                           16                   50.000000                49.860000                15.960000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Speedup: 50%                                                                                             \n          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     \n          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n          details on launch configurations.                                                                             \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores          16           2            32                 934.270000       40342944               49.940000              7.850000          43170.0                   99.870000                 8.470000        40287745                  11.110000         32     32768                  45                    0.032002                               0                         0.000000                               0  68.270000            16                   40                   16               32                           16                   50.000000                49.750000                15.920000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores          16           4            64                 932.570000       40363923               49.920000              6.830000          43270.0                   99.840000                 5.860000        40292636                  11.100000         64     16384                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   20                   16               16                           32                  100.000000                97.620000                31.240000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          16           8           128                 935.030000       40351285               49.920000              3.810000          43150.0                   99.840000                 2.870000        40276375                  11.110000        128      8192                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   10                   16                8                           32                  100.000000                98.670000                31.570000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          16          16           256                 936.700000       40424011               49.830000              1.980000          43150.0                   99.660000                 1.860000        40257144                  11.090000        256      4096                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    5                   16                4                           32                  100.000000                98.830000                31.620000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          16          32           512                 943.400000       40712695               49.470000              1.020000          43150.0                   98.940000                 1.380000        40258145                  11.010000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.140000                31.730000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          16          64          1024                 944.150000       41324784               48.730000              0.550000          43770.0                   97.470000                 1.140000        40294196                  10.840000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.760000                31.600000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          32           1            32                 954.480000       74160324               49.810000              4.190000          77670.0                   99.620000                13.540000        74028102                   6.040000         32     32768                  45                    0.032002                               0                         0.000000                               0  68.270000            16                   40                   16               32                           16                   50.000000                49.780000                15.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \nOPT   Est. Local Speedup: 50%                                                                                       \n          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       \n          hardware maximum of 8. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that    \n          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    \n          memory.                                                                                                       \n
./attention/build compute_attention_scores          32           2            64                 954.400000       74258914               49.730000              4.890000          77790.0                   99.470000                 9.350000        74075601                   6.040000         64     16384                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   20                   16               16                           32                  100.000000                97.300000                31.140000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          32           4           128                 948.110000       73979634               49.910000              3.760000          78020.0                   99.820000                 2.660000        73834566                   6.060000        128      8192                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   10                   16                8                           32                  100.000000                98.690000                31.580000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          32           8           256                 950.840000       74096173               49.820000              2.030000          77920.0                   99.650000                 1.280000        73763452                   6.050000        256      4096                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    5                   16                4                           32                  100.000000                98.910000                31.650000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          32          16           512                 955.680000       74627859               49.470000              1.080000          78080.0                   98.940000                 0.760000        73797356                   6.000000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.180000                31.740000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          32          32          1024                 955.610000       75726584               48.750000              0.580000          79240.0                   97.500000                 0.500000        73844294                   5.920000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.790000                31.610000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          64           1            64                 952.520000       74510017               49.590000              5.010000          78170.0                   99.180000                15.480000        74281042                   6.020000         64     16384                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   20                   16               16                           32                  100.000000                97.130000                31.080000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          64           2           128                 955.450000       74234183               49.740000              3.950000          77680.0                   99.490000                 6.550000        74078137                   6.040000        128      8192                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   10                   16                8                           32                  100.000000                98.740000                31.600000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          64           4           256                 950.500000       74107651               49.820000              2.110000          77960.0                   99.630000                 2.400000        73776082                   6.050000        256      4096                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    5                   16                4                           32                  100.000000                98.900000                31.650000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          64           8           512                 954.610000       74628884               49.470000              1.130000          78170.0                   98.940000                 1.130000        73798193                   6.000000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.190000                31.740000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores          64          16          1024                 957.140000       75706194               48.760000              0.610000          79090.0                   97.530000                 0.630000        73761885                   5.920000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.790000                31.610000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores         128           1           128                 949.700000       74442470               49.630000              4.230000          78340.0                   99.260000                14.410000        74291246                   6.020000        128      8192                  45                    0.032002                               0                         0.000000                               0  34.130000            16                   10                   16                8                           32                  100.000000                98.780000                31.610000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores         128           2           256                 950.870000       74391770               49.640000              2.060000          78210.0                   99.280000                 6.200000        74057501                   6.020000        256      4096                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    5                   16                4                           32                  100.000000                98.900000                31.650000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores         128           4           512                 955.050000       74632219               49.470000              1.140000          78130.0                   98.940000                 2.150000        73697188                   6.000000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.260000                31.760000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores         128           8          1024                 954.620000       74675998               49.440000              0.610000          78210.0                   98.890000                 1.300000        73687772                   6.000000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.790000                31.610000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores         256           1           256                 955.470000       74582926               49.550000              2.080000          77990.0                   99.110000                15.060000        74231811                   6.010000        256      4096                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    5                   16                4                           32                  100.000000                98.880000                31.640000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores         256           2           512                 948.790000       74380251               49.660000              1.130000          78360.0                   99.320000                 6.910000        73909147                   6.030000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.230000                31.750000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores         256           4          1024                 951.530000       75088081               49.280000              0.620000          78850.0                   98.550000                17.460000        74003731                   5.970000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.750000                31.600000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores         512           1           512                 954.420000       75128835               49.250000              1.130000          78640.0                   98.500000                19.780000        74210978                   5.970000        512      2048                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    2                   16                2                           32                  100.000000                99.190000                31.740000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores         512           2          1024                 954.200000       76024476               48.720000              0.610000          79670.0                   97.430000                40.130000        74953998                   5.890000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.700000                31.580000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build compute_attention_scores        1024           1          1024                 941.010000       75557624               55.100000              0.620000          80290.0                   98.080000                55.100000        74457744                   5.930000          1      1024                  45                    0.032002                               0                         0.000000                               0  34.130000            16                    1                   16                1                           32                  100.000000                98.810000                31.620000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 
./attention/build softmax_attention_scores           1           1             1                 926.450000        1914731               10.800000              2.980000           2040.0                   72.720000                10.800000          245418                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1           2             2                 937.260000        1901709               10.850000              3.000000           2000.0                   72.600000                10.850000          245775                   2.270000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1           4             4                 905.040000        1875365               11.020000              3.050000           2050.0                   72.970000                11.020000          244366                   2.300000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1           8             8                 924.430000        1869921               11.060000              3.050000           2000.0                   72.980000                11.060000          244564                   2.310000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1          16            16                 938.980000        1906361               10.820000              3.000000           2010.0                   72.470000                10.820000          246462                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1          32            32                 923.300000        1879579               10.960000              3.020000           2020.0                   72.740000                10.960000          245381                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1          64            64                 929.660000        1894142               10.830000              3.000000           2020.0                   72.870000                10.830000          244724                   2.270000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.830000                 7.950000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.17%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1         128           128                 934.680000        1900395               10.840000              2.990000           2010.0                   72.530000                10.840000          246369                   2.270000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1         256           256                 930.880000        1891536               10.920000              3.020000           2010.0                   72.000000                10.920000          247894                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1         512           512                 916.780000        1874450               11.030000              3.040000           2020.0                   72.300000                11.030000          247079                   2.300000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           1        1024          1024                 930.030000        1884313               10.960000              3.030000           2000.0                   72.680000                10.960000          245746                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2           1             2                 926.700000        1888837               10.880000              3.010000           2020.0                   72.300000                10.880000          246999                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2           2             4                 948.480000        1911320               10.780000              2.970000           2000.0                   72.410000                10.780000          246858                   2.250000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.820000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.18%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2           4             8                 927.590000        1889757               10.940000              3.010000           2020.0                   72.980000                10.940000          244652                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2           8            16                 930.820000        1885117               10.890000              3.010000           2010.0                   71.900000                10.890000          248088                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.820000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.18%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2          16            32                 929.680000        1905619               10.810000              2.990000           2030.0                   72.490000                10.810000          246418                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2          32            64                 924.260000        1872020               10.970000              3.040000           2010.0                   72.250000                10.970000          246964                   2.300000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2          64           128                 913.820000        1887209               10.980000              3.030000           2040.0                   72.000000                10.980000          247589                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2         128           256                 924.660000        1888835               10.930000              3.010000           2020.0                   71.750000                10.930000          248328                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2         256           512                 935.530000        1915194               10.790000              2.980000           2020.0                   72.040000                10.790000          247565                   2.250000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           2         512          1024                 933.480000        1913680               10.760000              2.980000           2030.0                   72.600000                10.760000          246193                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           4           1             4                 913.120000        1886936               10.960000              3.020000           2040.0                   72.030000                10.960000          247909                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           4           2             8                 914.320000        1859380               11.100000              3.060000           2010.0                   72.810000                11.100000          245296                   2.320000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.820000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.18%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           4           4            16                 919.830000        1881391               10.970000              3.030000           2020.0                   72.180000                10.970000          247301                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.770000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.23%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           4           8            32                 930.540000        1879103               10.980000              3.040000           2000.0                   72.310000                10.980000          246624                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           4          16            64                 929.610000        1911455               10.820000              2.990000           2030.0                   71.600000                10.820000          249330                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           4          32           128                 918.100000        1865457               11.080000              3.060000           2010.0                   72.240000                11.080000          246532                   2.320000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           4          64           256                 925.810000        1885643               10.980000              3.040000           2010.0                   72.690000                10.980000          245682                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           4         128           512                 924.180000        1875553               10.980000              3.030000           2010.0                   73.000000                10.980000          244757                   2.300000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.820000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.18%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           4         256          1024                 939.690000        1894196               10.880000              3.020000           1990.0                   72.270000                10.880000          246973                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.770000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.23%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           8           1             8                 942.820000        1909117               10.830000              2.990000           2000.0                   72.930000                10.830000          244893                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           8           2            16                 916.000000        1892357               10.930000              3.020000           2040.0                   72.420000                10.930000          246386                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           8           4            32                 934.730000        1920671               10.770000              2.970000           2030.0                   72.560000                10.770000          245929                   2.250000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.770000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.23%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           8           8            64                 933.080000        1882033               10.970000              3.030000           1990.0                   72.500000                10.970000          246440                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           8          16           128                 953.970000        1920642               10.750000              2.980000           1990.0                   73.230000                10.750000          243777                   2.250000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.820000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.18%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           8          32           256                 933.570000        1886133               10.940000              3.030000           1990.0                   72.330000                10.940000          246932                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           8          64           512                 942.280000        1905777               10.760000              2.990000           2000.0                   72.350000                10.760000          246955                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores           8         128          1024                 922.770000        1877834               11.010000              3.040000           2010.0                   72.130000                11.010000          247215                   2.300000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          16           1            16                 932.780000        1902202               10.880000              3.010000           2010.0                   72.430000                10.880000          246513                   2.270000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          16           2            32                 933.260000        1877058               11.010000              3.040000           1990.0                   72.770000                11.010000          245713                   2.300000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          16           4            64                 937.990000        1907089               10.840000              3.000000           2010.0                   72.880000                10.840000          244621                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          16           8           128                 919.710000        1892190               10.930000              3.020000           2030.0                   72.360000                10.930000          246678                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          16          16           256                 944.420000        1930275               10.690000              2.970000           2020.0                   72.480000                10.690000          246345                   2.240000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          16          32           512                 931.660000        1924914               10.760000              2.970000           2040.0                   72.800000                10.760000          245410                   2.250000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          16          64          1024                 928.430000        1877098               10.980000              3.030000           2000.0                   72.450000                10.980000          246343                   2.300000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          32           1            32                 920.150000        1894371               10.880000              3.010000           2030.0                   72.380000                10.880000          246237                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          32           2            64                 924.440000        1909038               10.830000              3.000000           2040.0                   72.250000                10.830000          247131                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          32           4           128                 918.620000        1892479               10.900000              3.010000           2040.0                   72.470000                10.900000          245936                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          32           8           256                 943.650000        1915701               10.820000              2.990000           2000.0                   72.780000                10.820000          245466                   2.250000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          32          16           512                 918.530000        1870884               11.040000              3.060000           2010.0                   72.350000                11.040000          246882                   2.310000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          32          32          1024                 948.190000        1935950               10.690000              2.950000           2010.0                   72.060000                10.690000          247597                   2.230000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.770000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.23%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          64           1            64                 914.670000        1885935               10.970000              3.040000           2030.0                   72.310000                10.970000          246906                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          64           2           128                 934.110000        1917892               10.780000              2.970000           2030.0                   72.890000                10.780000          244600                   2.250000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          64           4           256                 929.100000        1913612               10.800000              2.990000           2030.0                   71.620000                10.800000          248999                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          64           8           512                 908.150000        1873849               11.030000              3.040000           2040.0                   72.610000                11.030000          245976                   2.300000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.820000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.18%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores          64          16          1024                 909.050000        1874389               11.030000              3.040000           2040.0                   72.230000                11.030000          246780                   2.300000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores         128           1           128                 930.580000        1894821               10.900000              3.010000           2010.0                   72.360000                10.900000          246738                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores         128           2           256                 923.480000        1899261               10.920000              3.020000           2030.0                   72.370000                10.920000          247102                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores         128           4           512                 931.460000        1908153               10.840000              3.000000           2020.0                   71.930000                10.840000          247991                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.760000                 7.920000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.24%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores         128           8          1024                 930.060000        1915031               10.790000              2.990000           2030.0                   72.720000                10.790000          245644                   2.250000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores         256           1           256                 922.050000        1887545               10.980000              3.030000           2020.0                   72.210000                10.980000          247319                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.780000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.22%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores         256           2           512                 948.130000        1907692               10.830000              2.990000           1990.0                   72.370000                10.830000          246624                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores         256           4          1024                 940.990000        1910275               10.830000              3.000000           2010.0                   72.620000                10.830000          245694                   2.260000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores         512           1           512                 913.480000        1885498               10.970000              3.020000           2040.0                   72.530000                10.970000          246231                   2.290000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.790000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.21%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores         512           2          1024                 932.860000        1892364               10.920000              3.020000           2000.0                   72.360000                10.920000          246585                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.810000                 7.940000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.19%                                                                                    \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n
./attention/build softmax_attention_scores        1024           1          1024                 923.630000        1894777               10.920000              3.030000           2020.0                   72.450000                10.920000          246322                   2.280000        256         4                  41                    0.032002                               0                         0.000000                               0   0.030000            16                    5                   16                4                           32                  100.000000                24.800000                 7.930000         N/A                                                                                                                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                                                                                                     OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n          waves across all SMs. Look at Launch Statistics for more details.                                             \nOPT   Est. Speedup: 86.67%                                                                                          \n          The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 30              \n          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n          concurrently with other workloads, consider reducing the block size to have at least one block per            \n          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n          description for more details on launch configurations.                                                        \nOPT   Est. Local Speedup: 75.2%                                                                                     \n          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.8%) can be the     \n          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n          optimizing occupancy.                                                                                         \n